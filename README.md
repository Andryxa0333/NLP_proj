
# Описание проекта

Данный проект исследует метод **поэтапного обучения** с целью улучшения процесса обучения трансформеров на примере решения базовых математических операций. Суть метода заключается в том, что модель сначала обучается на простых задачах, а затем постепенно переходит к более сложным. Основная гипотеза состоит в том, что такой подход способствует более быстрой сходимости и повышению итогового качества модели по сравнению с обучением на данных, подаваемых в случайном порядке. Это позволяет модели сначала освоить базовые концепции, прежде чем приступать к решению более сложных задач. Эксперимент включает два этапа обучения:

1. **Стандартный подход**: данные подаются в случайном порядке.
2. **Поэтапное обучение**: данные подаются в порядке возрастания сложности.

В проекте используется **Transformer Decoder** для решения математических операций, таких как сложение, вычитание, умножение и возведение в квадрат. Процесс обучения при поэтапном подходе разделён на две фазы:

1. **Обучение на простых данных**: операции сложения и вычитания.
2. **Обучение на сложных данных**: операции умножения и возведения в квадрат.

Гипотеза заключается в том, что поэтапное обучение улучшит как скорость сходимости, так и качество модели.

# Цели проекта

- Исследовать влияние поэтапного обучения на решение математических задач.
- Реализовать трансформер для выполнения простых и сложных математических операций.
- Разработать кастомный токенизатор для обработки чисел и операторов в математических выражениях.
- Провести эксперимент с разными стратегиями подачи данных (поэтапная и случайная).
- Оценить результаты обучения и сходимость модели.
# Реализация
## Генерация данных

Для создания датасета используется скрипт `data_generator.py`. Результаты сохраняются в файл `curriculum_data.csv`, который содержит как простые, так и сложные операции.


## Токенизатор
Токенизатор **tokenizer** преобразует текстовые выражения в последовательности токенов, которые используются для обучения модели. Основные токены включают:

- `<PAD>`: токен для дополнения последовательностей до одинаковой длины. 
- `<SOS>`: токен начала последовательности. 
- `<EOS>`: токен конца последовательности. 
- `<UNK>`: токен для неизвестных символов. 
- Математические операторы: `+`: 4, `-`: 5, `*`: 6, `^`: 7, `=`: 8. 
- Цифры: от 0 до 9, которым соответствуют токены от 10 до 19.

## Модель
Модель основана на **Transformer Decoder** и обрабатывает последовательности токенов, предсказывая результаты математических операций. Основные параметры модели:
- **num_tokens**: количество уникальных токенов. 
- **n_embd**: размерность эмбеддингов. 
- **num_layers**: количество слоев декодера. 
- **num_heads**: количество голов внимания. 
- **num_classes**: количество возможных результатов операций. 

Обучение модели происходит в два этапа:

- **Случайное сэмплирование**: данные подаются в случайном порядке. 
- **Поэтапное обучение**: данные подаются в порядке возрастания сложности.


# Результаты эксперимента

![image](![photo_2025-03-11_03-04-03](https://github.com/user-attachments/assets/3c55b0c7-93f0-430d-b580-fd1ff6e80cb7))

Графики показывают динамику **Loss** и **Accuracy** для обоих подходов.

- **Потери на тренировочной выборке** снижаются быстрее при **поэтапном обучении** (зеленая линия), показывая более эффективную подгонку модели в сравнении со случайным подходом (синяя линия). Однако **Val Loss** (красная линия) остаётся на высоком уровне, что может указывать на проблемы с генерализацией.
- **Точность на тренировочной выборке** при **поэтапном обучении** достигает 25% к 10-й эпохе, что заметно выше, чем при случайном сэмплировании, где показател остаётся на уровне 15%. В то же время **валидационная точность** (оранжевая линия) увеличивается, но остаётся на уровне 15-20% и не демонстрирует значительных улучшений.

# Выводы

Эксперимент подтвердил, что **поэтапное обучение** действительно ускоряет сходимость модели и улучшает точность на тренировочных данных. Однако на валидационных данных этот подход не продемонстрировал значительных улучшений, что может быть связано с переобучением или недостаточной способностью модели обобщать. Таким образом, хотя поэтапное обучение эффективно для ускорения процесса обучения, его влияние на качество предсказаний на новых данных остаётся неясным и требует дальнейшего изучения.

# Запуск эксперимента

1. Создайте виртуальное окружение и установите зависимости:
```bash
pip install -r "requirements.txt"
```
2. Основной процесс обучения запускается через файл `main.py`:
```bash
python main.py
```

3. Конфигурация эксперимента задаётся в файле `config.json`.
