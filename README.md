
# Описание проекта

Данный проект исследует метод **поэтапного обучения** с целью улучшения процесса обучения трансформеров на примере решения базовых математических операций. Суть метода заключается в том, что модель сначала обучается на простых задачах, а затем постепенно переходит к более сложным. Основная гипотеза состоит в том, что такой подход способствует более быстрой сходимости и повышению итогового качества модели по сравнению с обучением на данных, подаваемых в случайном порядке. Это позволяет модели сначала освоить базовые концепции, прежде чем приступать к решению более сложных задач. Эксперимент включает два этапа обучения:

1. **Стандартный подход**: данные подаются в случайном порядке.
2. **Поэтапное обучение**: данные подаются в порядке возрастания сложности.

В проекте используется **Transformer Decoder** для решения математических операций, таких как сложение, вычитание, умножение и возведение в квадрат. Процесс обучения при поэтапном подходе разделён на две фазы:

1. **Обучение на простых данных**: операции сложения и вычитания.
2. **Обучение на сложных данных**: операции умножения и возведения в квадрат.

Гипотеза заключается в том, что поэтапное обучение улучшит как скорость сходимости, так и качество модели.

# Цели проекта

- Исследовать влияние поэтапного обучения на решение математических задач.
- Реализовать трансформер для выполнения простых и сложных математических операций.
- Разработать кастомный токенизатор для обработки чисел и операторов в математических выражениях.
- Провести эксперимент с разными стратегиями подачи данных (поэтапная и случайная).
- Оценить результаты обучения и сходимость модели.
# Реализация
## Генерация данных

Для создания датасета используется скрипт `data_generator.py`. Результаты сохраняются в файл `curriculum_data.csv`, который содержит как простые, так и сложные операции.


## Токенизатор
Токенизатор **tokenizer** преобразует текстовые выражения в последовательности токенов, которые используются для обучения модели. Основные токены включают:

- `<PAD>`: токен для дополнения последовательностей до одинаковой длины. 
- `<SOS>`: токен начала последовательности. 
- `<EOS>`: токен конца последовательности. 
- `<UNK>`: токен для неизвестных символов. 
- Математические операторы: `+`: 4, `-`: 5, `*`: 6, `^`: 7, `=`: 8. 
- Цифры: от 0 до 9, которым соответствуют токены от 10 до 19.

## Модель
Модель основана на **Transformer Decoder** и обрабатывает последовательности токенов, предсказывая результаты математических операций. Основные параметры модели:
- **num_tokens**: количество уникальных токенов. 
- **n_embd**: размерность эмбеддингов. 
- **num_layers**: количество слоев декодера. 
- **num_heads**: количество голов внимания. 
- **num_classes**: количество возможных результатов операций. 

Обучение модели происходит в два этапа:

- **Случайное сэмплирование**: данные подаются в случайном порядке. 
- **Поэтапное обучение**: данные подаются в порядке возрастания сложности.


# Результаты эксперимента

![photo_2025-03-11_02-59-44](https://github.com/user-attachments/assets/3e8b1ee1-55c3-44ad-9262-be7c27f7cbae)


Графики показывают динамику **Loss** и **Accuracy** для обоих подходов.

- **Потери на тренировочной выборке** (синяя линия) для случайного подхода показывают медленное снижение, в то время как **поэтапное обучение** (зелёная линия) демонстрирует более быстрое снижение **Curriculum Train Loss**. Однако **Val Loss** (красная линия) при поэтапном обучении остается значительно высоком, что может указывать на проблемы с генерализацией несмотря на сильное снижение на обучающей выборке.
- **Точность на тренировочной выборке** при **поэтапном обучении** (зелёная линия) достигает 85% к 50-й эпохе, что значительно выше по сравнению с **случайным подходом** (синяя линия), где точность достигает только 60%. В то же время **валидационная точность** (оранжевая линия) увеличивается, но остается на уровне 30-40%, не демонстрируя значительных улучшений по сравнению с точностью на обучающей выборке.

# Выводы

Эксперимент подтвердил, что **поэтапное обучение** существенно улучшает сходимость модели и точность на тренировочных данных. Однако на валидационных данных этот подход не продемонстрировал значительных улучшений, что может быть связано с переобучением или недостаточной способностью модели обобщать. Следовательно, несмотря на явные преимущества в обучении, необходимо дальнейшее исследование для понимания влияния данного подхода на качество предсказаний на новых данных.

# Запуск эксперимента

1. Создайте виртуальное окружение и установите зависимости:
```bash
pip install -r "requirements.txt"
```
2. Основной процесс обучения запускается через файл `main.py`:
```bash
python main.py
```

3. Конфигурация эксперимента задаётся в файле `config.json`.
