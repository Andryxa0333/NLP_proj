
# Описание проекта

Данный проект исследует метод **поэтапного обучения** с целью улучшения процесса обучения трансформеров на примере решения базовых математических операций. Суть метода заключается в том, что модель сначала обучается на простых задачах, а затем постепенно переходит к более сложным. Основная гипотеза состоит в том, что такой подход способствует более быстрой сходимости и повышению итогового качества модели по сравнению с обучением на данных, подаваемых в случайном порядке. Это позволяет модели сначала освоить базовые концепции, прежде чем приступать к решению более сложных задач. Эксперимент включает два этапа обучения:

1. **Стандартный подход**: данные подаются в случайном порядке.
2. **Поэтапное обучение**: данные подаются в порядке возрастания сложности.

В проекте используется **Transformer Decoder** для решения математических операций, таких как сложение, вычитание, умножение и возведение в квадрат. Процесс обучения при поэтапном подходе разделён на две фазы:

1. **Обучение на простых данных**: операции сложения и вычитания.
2. **Обучение на сложных данных**: операции умножения и возведения в квадрат.

Гипотеза заключается в том, что поэтапное обучение улучшит как скорость сходимости, так и качество модели.

# Цели проекта

- Исследовать влияние поэтапного обучения на решение математических задач.
- Реализовать трансформер для выполнения простых и сложных математических операций.
- Разработать кастомный токенизатор для обработки чисел и операторов в математических выражениях.
- Провести эксперимент с разными стратегиями подачи данных (поэтапная и случайная).
- Оценить результаты обучения и сходимость модели.
# Реализация
## Генерация данных

Для создания датасета используется скрипт `data_generator.py`. Результаты сохраняются в файл `curriculum_data.csv`, который содержит как простые, так и сложные операции.


## Токенизатор
Токенизатор **tokenizer** преобразует текстовые выражения в последовательности токенов, которые используются для обучения модели. Основные токены включают:

- `<PAD>`: токен для дополнения последовательностей до одинаковой длины. 
- `<SOS>`: токен начала последовательности. 
- `<EOS>`: токен конца последовательности. 
- `<UNK>`: токен для неизвестных символов. 
- Математические операторы: `+`: 4, `-`: 5, `*`: 6, `^`: 7, `=`: 8. 
- Цифры: от 0 до 9, которым соответствуют токены от 10 до 19.

## Модель
Модель основана на **Transformer Decoder** и обрабатывает последовательности токенов, предсказывая результаты математических операций. Основные параметры модели:
- **num_tokens**: количество уникальных токенов. 
- **n_embd**: размерность эмбеддингов. 
- **num_layers**: количество слоев декодера. 
- **num_heads**: количество голов внимания. 
- **num_classes**: количество возможных результатов операций. 

Обучение модели происходит в два этапа:

- **Случайное сэмплирование**: данные подаются в случайном порядке. 
- **Поэтапное обучение**: данные подаются в порядке возрастания сложности.


# Результаты эксперимента

![photo_2025-03-12_01-23-41](https://github.com/user-attachments/assets/7f15a15e-83c0-4c60-92a2-0e3d5d6b5012)



Графики показывают динамику **Loss** и **Accuracy** для обоих подходов.

- **Потери на тренировочной выборке** (синяя линия) для случайного подхода показывают медленное снижение, в то время как **поэтапное обучение** (зелёная линия) также демонстрирует снижение, но с меньшим уровнем потерь на протяжении обучения. **Val Loss** (красная линия) для поэтапного обучения начинает выше, чем у случайного подхода, но также показывают постепенное снижение, что может сигнализировать о лучшей сходимости модели.
- **Точность на тренировочной выборке** при **поэтапном обучении** (зелёная линия) постепенно растёт, достигая около 50% к 100-й эпохе, что заметно выше, чем у **случайного подхода** (синяя линия), где точность останавливается ниже 50%. **Валидационная точность** (оранжевая линия) растёт, хотя её максимальное значение остается на уровне 30-40%, что указывает на недостаточную способность модели обобщать.

# Выводы

Эксперимент подтверждает, что **поэтапное обучение** не только способствует лучшей сходимости, но и позволяет добиться лучшей точности на тренировочных данных. Однако на валидационных данных результаты остаются ограниченными, что может сигнализировать о переобучении или недостаточной обобщающей способности модели. Поэтому, несмотря на заметные преимущества в тренировочном процессе, требуется дальнейшее исследование для улучшения качества предсказаний на ранее невидимых данных.

# Запуск эксперимента

1. Создайте виртуальное окружение и установите зависимости:
```bash
pip install -r "requirements.txt"
```
2. Основной процесс обучения запускается через файл `main.py`:
```bash
python main.py
```

3. Конфигурация эксперимента задаётся в файле `config.json`.
